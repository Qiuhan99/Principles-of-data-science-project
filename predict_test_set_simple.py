"""
ç®€åŒ–çš„æµ‹è¯•é›†é¢„æµ‹è„šæœ¬
å¯ä»¥ç›´æ¥å¯¼å…¥è®­ç»ƒè„šæœ¬ä¸­çš„æ¨¡å‹ç±»ï¼Œé¿å…é‡å¤å®šä¹‰

ä½¿ç”¨æ–¹æ³•:
    python predict_test_set_simple.py --test_dir /path/to/test --model_path model.pth
"""

import os
import sys
import glob
import argparse
import numpy as np
import pandas as pd
from PIL import Image
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

# å°è¯•ä»è®­ç»ƒè„šæœ¬å¯¼å…¥æ¨¡å‹ç±»
try:
    # å°è¯•å¯¼å…¥Colabç‰ˆæœ¬
    sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
    from dual_path_analysis_colab import DualPathModel, SwinBackbone, NUM_CLASSES, CLASSES as TRAIN_CLASSES
    print("âœ“ ä» dual_path_analysis_colab å¯¼å…¥æ¨¡å‹ç±»")
except ImportError:
    try:
        # å°è¯•å¯¼å…¥æ ‡å‡†ç‰ˆæœ¬
        from dual_path_analysis import DualPathModel, SwinBackbone, NUM_CLASSES, CLASSES as TRAIN_CLASSES
        print("âœ“ ä» dual_path_analysis å¯¼å…¥æ¨¡å‹ç±»")
    except ImportError:
        print("è­¦å‘Š: æ— æ³•å¯¼å…¥è®­ç»ƒè„šæœ¬ï¼Œå°†ä½¿ç”¨å†…ç½®æ¨¡å‹å®šä¹‰")
        # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨å†…ç½®å®šä¹‰ï¼ˆç®€åŒ–ç‰ˆï¼‰
        from transformers import SwinModel, SwinConfig
        import torch.nn as nn
        
        class SwinBackbone(nn.Module):
            def __init__(self, model_name='microsoft/swin-tiny-patch4-window7-224'):
                super().__init__()
                config = SwinConfig.from_pretrained(model_name)
                self.swin = SwinModel.from_pretrained(model_name, config=config)
            
            def forward(self, x):
                outputs = self.swin(x)
                if hasattr(outputs, 'last_hidden_state'):
                    return outputs.last_hidden_state.mean(dim=1)
                return outputs.pooler_output
        
        class DualPathModel(nn.Module):
            def __init__(self, num_classes=7):
                super().__init__()
                self.global_backbone = SwinBackbone()
                self.local_backbone = SwinBackbone()
                self.global_backbone.eval()
                with torch.no_grad():
                    dummy_input = torch.randn(1, 3, 224, 224)
                    feature_dim = self.global_backbone(dummy_input).shape[1]
                self.global_classifier = nn.Linear(feature_dim, num_classes)
                self.local_classifier = nn.Linear(feature_dim, num_classes)
            
            def forward(self, full_image, local_image=None):
                global_features = self.global_backbone(full_image)
                global_logits = self.global_classifier(global_features)
                if local_image is None:
                    local_image = full_image
                local_features = self.local_backbone(local_image)
                local_logits = self.local_classifier(local_features)
                return global_logits, local_logits, global_features, local_features
        
        TRAIN_CLASSES = ['ALB', 'BET', 'DOL', 'LAG', 'OTHER', 'SHARK', 'YFT']
        NUM_CLASSES = len(TRAIN_CLASSES)

# æäº¤æ—¶çš„ç±»åˆ«ï¼ˆ8ä¸ªï¼ŒåŒ…æ‹¬NoFï¼‰
SUBMIT_CLASSES = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']

# Colabç¯å¢ƒæ£€æµ‹
try:
    import google.colab
    IN_COLAB = True
    BASE_DIR = '/content'
except ImportError:
    IN_COLAB = False
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))


class TestDataset(Dataset):
    """æµ‹è¯•é›†æ•°æ®é›†ç±»"""
    
    def __init__(self, test_dir: str, transform=None):
        self.test_dir = test_dir
        self.transform = transform
        self.image_files = []
        
        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.JPG', '.JPEG', '.PNG']
        
        # æœç´¢æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        if os.path.isdir(test_dir):
            for ext in image_extensions:
                pattern1 = os.path.join(test_dir, f'*{ext}')
                pattern2 = os.path.join(test_dir, '**', f'*{ext}')
                self.image_files.extend(glob.glob(pattern1))
                self.image_files.extend(glob.glob(pattern2, recursive=True))
        elif os.path.isfile(test_dir):
            self.image_files = [test_dir]
        
        # å»é‡å¹¶æ’åº
        self.image_files = sorted(list(set(self.image_files)))
        
        if not self.image_files:
            print(f"âš ï¸  è­¦å‘Š: åœ¨ {test_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡æ–‡ä»¶")
        else:
            print(f"âœ“ æ‰¾åˆ° {len(self.image_files)} å¼ æµ‹è¯•å›¾ç‰‡")
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        img_path = self.image_files[idx]
        img_filename = os.path.basename(img_path)
        
        try:
            image = Image.open(img_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
            return {'image': image, 'filename': img_filename, 'full_path': img_path}
        except Exception as e:
            print(f"âš ï¸  è­¦å‘Š: æ— æ³•åŠ è½½å›¾ç‰‡ {img_path}: {e}")
            placeholder = Image.new('RGB', (224, 224), color='gray')
            if self.transform:
                placeholder = self.transform(placeholder)
            return {'image': placeholder, 'filename': img_filename, 'full_path': img_path}


def load_model(model_path: str, device='cuda'):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
    
    # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
    possible_paths = [
        model_path,
        os.path.join(BASE_DIR, model_path),
        os.path.join('/content', model_path),
        os.path.join('/content/drive/MyDrive', model_path),
        os.path.join('//content/drive/MyDrive/fish dataset/test_stg1', model_path),
    ]
    
    actual_path = None
    for path in possible_paths:
        if os.path.exists(path):
            actual_path = path
            break
    
    if actual_path is None:
        print("âŒ é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        print("å°è¯•è¿‡çš„è·¯å¾„:")
        for path in possible_paths:
            print(f"  - {path}")
        return None
    
    print(f"âœ“ æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {actual_path}")
    
    # åˆ›å»ºæ¨¡å‹
    model = DualPathModel(num_classes=NUM_CLASSES)
    
    # åŠ è½½æƒé‡
    try:
        state_dict = torch.load(actual_path, map_location=device)
        model.load_state_dict(state_dict)
        print("âœ“ æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ é”™è¯¯: åŠ è½½æ¨¡å‹æƒé‡å¤±è´¥: {e}")
        return None
    
    model.to(device)
    model.eval()
    return model


def predict_test_set(model, test_loader, device='cuda', use_ensemble=True):
    """å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹"""
    model.eval()
    predictions = {}
    
    print(f"\nğŸ”® å¼€å§‹é¢„æµ‹ (ä½¿ç”¨{'é›†æˆ' if use_ensemble else 'å…¨å±€'}é¢„æµ‹)...")
    with torch.no_grad():
        for batch_idx, batch in enumerate(test_loader):
            images = batch['image'].to(device)
            filenames = batch['filename']
            
            # å‰å‘ä¼ æ’­ï¼ˆæµ‹è¯•é›†æ²¡æœ‰BBoxï¼Œä½¿ç”¨æ•´å¼ å›¾ç‰‡ä½œä¸ºå…¨å±€å’Œå±€éƒ¨ï¼‰
            global_logits, local_logits, _, _ = model(images, images)
            
            # è½¬æ¢ä¸ºæ¦‚ç‡
            global_probs = F.softmax(global_logits, dim=1)
            local_probs = F.softmax(local_logits, dim=1)
            
            # é›†æˆé¢„æµ‹
            if use_ensemble:
                ensemble_probs = (global_probs + local_probs) / 2.0
            else:
                ensemble_probs = global_probs
            
            # å­˜å‚¨é¢„æµ‹ç»“æœ
            probs_np = ensemble_probs.cpu().numpy()
            for i, filename in enumerate(filenames):
                predictions[filename] = probs_np[i]
            
            if (batch_idx + 1) % 10 == 0:
                print(f"  è¿›åº¦: {batch_idx + 1}/{len(test_loader)} æ‰¹æ¬¡")
    
    print(f"âœ“ é¢„æµ‹å®Œæˆï¼Œå…± {len(predictions)} å¼ å›¾ç‰‡")
    return predictions


def convert_to_submit_format(predictions: dict) -> pd.DataFrame:
    """å°†é¢„æµ‹ç»“æœè½¬æ¢ä¸ºæäº¤æ ¼å¼"""
    rows = []
    
    for filename, probs in predictions.items():
        # åˆ›å»º8ä¸ªç±»åˆ«çš„æ¦‚ç‡æ•°ç»„
        submit_probs = np.zeros(8)
        
        # æ˜ å°„7ä¸ªè®­ç»ƒç±»åˆ«åˆ°8ä¸ªæäº¤ç±»åˆ«
        for i, train_class in enumerate(TRAIN_CLASSES):
            if train_class in SUBMIT_CLASSES:
                submit_idx = SUBMIT_CLASSES.index(train_class)
                submit_probs[submit_idx] = probs[i]
        
        # NoFæ¦‚ç‡ï¼š1 - max(å…¶ä»–ç±»åˆ«æ¦‚ç‡)
        max_other_prob = np.max(submit_probs[:4]) + np.max(submit_probs[5:])
        nof_prob = max(0.0, min(1.0, 1.0 - max_other_prob))
        submit_probs[4] = nof_prob
        
        # å½’ä¸€åŒ–
        submit_probs = submit_probs / (submit_probs.sum() + 1e-10)
        
        # æ¦‚ç‡è£å‰ª
        submit_probs = np.clip(submit_probs, 1e-15, 1 - 1e-15)
        
        rows.append({
            'å›¾åƒ': filename,
            'ALB': submit_probs[0],
            'BET': submit_probs[1],
            'DOL': submit_probs[2],
            'LAG': submit_probs[3],
            'NoF': submit_probs[4],
            'å…¶ä»–': submit_probs[5],
            'SHARK': submit_probs[6],
            'YFT': submit_probs[7]
        })
    
    df = pd.DataFrame(rows)
    column_order = ['å›¾åƒ', 'ALB', 'BET', 'DOL', 'LAG', 'NoF', 'å…¶ä»–', 'SHARK', 'YFT']
    return df[column_order]


def main():
    parser = argparse.ArgumentParser(description='æµ‹è¯•é›†é¢„æµ‹è„šæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼‰')
    parser.add_argument('--test_dir', type=str, required=True,
                        help='æµ‹è¯•å›¾ç‰‡ç›®å½•è·¯å¾„')
    parser.add_argument('--model_path', type=str, default='dual_path_model.pth',
                        help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--output', type=str, default='submission.csv',
                        help='è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--no_ensemble', action='store_true',
                        help='ä¸ä½¿ç”¨é›†æˆé¢„æµ‹')
    
    args = parser.parse_args()
    
    # æ£€æµ‹è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    if device.type == 'cuda':
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
    
    # åŠ è½½æ¨¡å‹
    model = load_model(args.model_path, device)
    if model is None:
        return
    
    # å›¾åƒå˜æ¢
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    print(f"\nğŸ“‚ åŠ è½½æµ‹è¯•é›†: {args.test_dir}")
    test_dataset = TestDataset(args.test_dir, transform=transform)
    
    if len(test_dataset) == 0:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•æµ‹è¯•å›¾ç‰‡")
        return
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    test_loader = DataLoader(
        test_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=0 if IN_COLAB else 2,
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    # è¿›è¡Œé¢„æµ‹
    predictions = predict_test_set(
        model, test_loader, device, 
        use_ensemble=not args.no_ensemble
    )
    
    # è½¬æ¢ä¸ºæäº¤æ ¼å¼
    print("\nğŸ“ ç”Ÿæˆæäº¤æ–‡ä»¶...")
    submission_df = convert_to_submit_format(predictions)
    
    # ä¿å­˜CSVæ–‡ä»¶
    output_path = args.output
    submission_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"âœ“ æäº¤æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_path}")
    print(f"  å…± {len(submission_df)} å¼ å›¾ç‰‡")
    
    # æ˜¾ç¤ºé¢„è§ˆ
    print("\nğŸ“Š å‰5è¡Œé¢„è§ˆ:")
    print(submission_df.head().to_string())
    
    # éªŒè¯æ¦‚ç‡å’Œ
    prob_sums = submission_df.iloc[:, 1:].sum(axis=1)
    print(f"\nâœ… æ¦‚ç‡å’Œæ£€æŸ¥:")
    print(f"  æœ€å°: {prob_sums.min():.6f}")
    print(f"  æœ€å¤§: {prob_sums.max():.6f}")
    print(f"  å¹³å‡: {prob_sums.mean():.6f}")
    
    if IN_COLAB:
        print("\nğŸ’¡ Colabæç¤º:")
        print("  ä¸‹è½½æ–‡ä»¶: from google.colab import files; files.download('submission.csv')")


if __name__ == '__main__':
    main()

